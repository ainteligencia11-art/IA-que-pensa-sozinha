import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Carregar o tokenizer
tokenizer = AutoTokenizer.from_pretrained("./Phi-3-mini-4k-instruct")

# Carregar a configura칞칚o do modelo
from transformers import Phi3Config
config = Phi3Config.from_pretrained("./Phi-3-mini-4k-instruct")
print("Configura칞칚o do modelo:")
print(config)

# Tentar carregar o modelo com baixo uso de mem칩ria
model = AutoModelForCausalLM.from_pretrained(
    "./Phi-3-mini-4k-instruct",
    torch_dtype=torch.float16,  # Usar meia precis칚o para economizar mem칩ria
    device_map="auto",          # Usar CPU (j치 que n칚o temos GPU poderosa)
    trust_remote_code=True      # Necess치rio para modelos Phi-3
)

# Inspecionar os tensores
print("\n游댌 Estrutura do modelo (primeiros 10 tensores):")
for name, param in model.named_parameters():
    print(f"{name}: {param.shape} {param.dtype}")
    if 'weight' in name:
        print(f"   Exemplo de valores: {param.data[0][:5]}")
    break  # S칩 mostra os primeiros 10 para n칚o sobrecarregar

# Verificar o n칰mero total de par칙metros
total_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal de par칙metros: {total_params:,}")