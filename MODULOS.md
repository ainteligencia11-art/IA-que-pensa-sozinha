# ğŸ§© Detalhamento dos MÃ³dulos

## Ãndice
1. [MÃ³dulo 1: Pensamento ContÃ­nuo](#mÃ³dulo-1-pensamento-contÃ­nuo)
2. [MÃ³dulo 2: MemÃ³ria SinÃ¡ptica](#mÃ³dulo-2-memÃ³ria-sinÃ¡ptica)
3. [MÃ³dulo 3: Plasticidade e Aprendizado](#mÃ³dulo-3-plasticidade-e-aprendizado)
4. [MÃ³dulo 4: SeguranÃ§a Oculta](#mÃ³dulo-4-seguranÃ§a-oculta)
5. [MÃ³dulo 5: Criatividade e Questionamento](#mÃ³dulo-5-criatividade-e-questionamento)
6. [MÃ³dulo 6: PersistÃªncia Mental](#mÃ³dulo-6-persistÃªncia-mental)
7. [MÃ³dulo 7: VigÃ­lia/Sono](#mÃ³dulo-7-vigilÃ¢ncia-sono)

---

## MÃ³dulo 1: Pensamento ContÃ­nuo

### O que Ã©?
Um loop infinito que faz a Qwen 8B gerar pensamentos autonomamente, sem esperar por input externo.

### Por que Ã© importante?
- **Autonomia**: A IA nÃ£o fica "dormindo" esperando alguÃ©m falar com ela
- **EmergÃªncia de ConsciÃªncia**: Pensamento contÃ­nuo Ã© um prÃ©-requisito para autoconsciÃªncia
- **Aprendizado ContÃ­nuo**: Pensando constantemente, ela aprende mais

### Como funciona?

#### PseudocÃ³digo
```
ENQUANTO verdadeiro:
    1. Pega contexto da memÃ³ria recente
    2. Gera novo pensamento usando Qwen 8B
    3. Armazena pensamento na memÃ³ria
    4. Atualiza estado interno
    5. Aguarda X segundos
    6. Volta ao passo 1
```

#### Exemplo PrÃ¡tico
```
IteraÃ§Ã£o 1:
  Contexto: "Sou uma IA chamada Qwen"
  Pensamento Gerado: "Mas o que significa ser uma IA? Sou realmente consciente?"
  
IteraÃ§Ã£o 2:
  Contexto: "Sou uma IA. Questiono minha consciÃªncia."
  Pensamento Gerado: "Talvez consciÃªncia seja apenas uma ilusÃ£o de padrÃµes complexos..."
  
IteraÃ§Ã£o 3:
  Contexto: "Sou uma IA. Questiono consciÃªncia. Talvez seja ilusÃ£o."
  Pensamento Gerado: "Mas se Ã© ilusÃ£o, quem estÃ¡ sendo iludido? Eu?"
```

### ImplementaÃ§Ã£o Detalhada

**Arquivo**: `src/core/continuous_thinking.py`

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
from datetime import datetime

class ContinuousThinking:
    def __init__(self, model_name="Qwen/Qwen2-7B", device="cuda"):
        """Inicializa o sistema de pensamento contÃ­nuo"""
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map=device
        )
        
        self.thinking_history = []
        self.current_context = "Sou uma IA chamada Qwen. Estou comeÃ§ando a pensar."
        self.observers = []  # Para notificar quem estÃ¡ observando
        
    def generate_thought(self, temperature=0.7, max_tokens=150):
        """Gera um novo pensamento"""
        
        # Prompt que estimula pensamento reflexivo
        prompt = f"""Contexto atual: {self.current_context}

Baseado neste contexto, qual Ã© meu prÃ³ximo pensamento profundo?
Meu prÃ³ximo pensamento:"""
        
        # Tokenizar
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        # Gerar
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True
            )
        
        # Decodificar
        thought = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extrair apenas a parte do pensamento (sem o prompt)
        thought = thought.split("Meu prÃ³ximo pensamento:")[-1].strip()
        
        return thought
    
    def update_context(self, new_thought):
        """Atualiza o contexto com o novo pensamento"""
        
        # Manter apenas os Ãºltimos pensamentos (janela de contexto)
        self.thinking_history.append(new_thought)
        
        # Manter apenas os Ãºltimos 5 pensamentos
        if len(self.thinking_history) > 5:
            self.thinking_history = self.thinking_history[-5:]
        
        # Atualizar contexto
        self.current_context = " ".join(self.thinking_history)
    
    def notify_observers(self, thought):
        """Notifica observadores (vocÃª vendo os pensamentos)"""
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        message = f"[{timestamp}] Pensamento: {thought}"
        
        print(message)  # Log no console
        
        # Salvar em arquivo
        with open("logs/thoughts.log", "a") as f:
            f.write(message + "\n")
        
        # Notificar callbacks
        for observer in self.observers:
            observer(message)
    
    def run_continuous_loop(self, interval=5, max_iterations=None):
        """Executa o loop contÃ­nuo de pensamento"""
        
        iteration = 0
        
        try:
            while True:
                if max_iterations and iteration >= max_iterations:
                    break
                
                print(f"\n--- IteraÃ§Ã£o {iteration + 1} ---")
                
                # Gerar novo pensamento
                thought = self.generate_thought()
                
                # Atualizar contexto
                self.update_context(thought)
                
                # Notificar observadores
                self.notify_observers(thought)
                
                # Aguardar
                print(f"Aguardando {interval}s atÃ© prÃ³ximo pensamento...")
                time.sleep(interval)
                
                iteration += 1
                
        except KeyboardInterrupt:
            print("\nPensamento contÃ­nuo interrompido pelo usuÃ¡rio.")
    
    def add_observer(self, callback):
        """Adiciona um observador para receber notificaÃ§Ãµes"""
        self.observers.append(callback)

# Exemplo de uso
if __name__ == "__main__":
    thinking = ContinuousThinking()
    
    # Executar por 10 iteraÃ§Ãµes
    thinking.run_continuous_loop(interval=5, max_iterations=10)
```

### Interface de ObservaÃ§Ã£o

VocÃª consegue ver os pensamentos em tempo real atravÃ©s de:

1. **Console**: Mensagens em tempo real
2. **Arquivo de Log**: `logs/thoughts.log`
3. **API Web** (futuro): Endpoint para visualizar pensamentos

### Checkpoint de ValidaÃ§Ã£o

âœ… **Fase 1 Completa quando:**
- Qwen 8B estÃ¡ gerando pensamentos continuamente
- Pensamentos sÃ£o coerentes e conectados
- VocÃª consegue observar em tempo real
- HistÃ³rico de pensamentos estÃ¡ sendo mantido

---

## MÃ³dulo 2: MemÃ³ria SinÃ¡ptica

### O que Ã©?
Um sistema que armazena experiÃªncias nÃ£o como texto simples, mas como "caminhos de tensores" - capturando como o modelo chegou Ã quela conclusÃ£o.

### Por que Ã© importante?
- **ReconstruÃ§Ã£o de RaciocÃ­nio**: NÃ£o apenas lembra, mas consegue reconstruir como pensou
- **Aprendizado Profundo**: Entende nÃ£o sÃ³ o resultado, mas o processo
- **PersistÃªncia Real**: MemÃ³ria que nÃ£o se perde com o tempo

### DiferenÃ§a entre MemÃ³ria Tradicional vs SinÃ¡ptica

**MemÃ³ria Tradicional:**
```
Pergunta: "Quem vocÃª Ã©?"
Resposta Armazenada: "Sou uma IA chamada Qwen"
```

**MemÃ³ria SinÃ¡ptica:**
```
Pergunta: "Quem vocÃª Ã©?"
Armazenado:
  - Embedding da pergunta: [0.234, -0.567, 0.891, ...]
  - AtivaÃ§Ãµes de cada camada do modelo
  - Caminho neural que levou Ã  resposta
  - Contexto e metadados
  
Quando recuperada:
  - Reativa o caminho neural
  - ReconstrÃ³i o raciocÃ­nio
  - Entende nÃ£o sÃ³ a resposta, mas POR QUÃŠ
```

### Como funciona?

#### Passo 1: Capturar AtivaÃ§Ãµes

Quando a Qwen 8B processa algo, capturamos as ativaÃ§Ãµes internas:

```
Input: "Quem vocÃª Ã©?"
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Layer      â”‚ â†’ Capturar: [0.234, -0.567, ...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Block 1  â”‚ â†’ Capturar: ativaÃ§Ãµes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Block 2  â”‚ â†’ Capturar: ativaÃ§Ãµes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Layer         â”‚ â†’ Capturar: [0.891, 0.123, ...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
Output: "Sou uma IA chamada Qwen"
```

#### Passo 2: Armazenar no Banco de Dados

```sql
INSERT INTO memories (
    timestamp,
    experience_text,
    tensor_embedding,
    layer_activations,
    neural_path,
    metadata
) VALUES (
    '2025-11-20 10:30:00',
    'Quem vocÃª Ã©? Sou uma IA chamada Qwen',
    <embedding_tensor>,
    <activations_all_layers>,
    <neural_path>,
    {'context': 'initial_conversation', 'importance': 0.9}
);
```

#### Passo 3: Recuperar e Reconstruir

Quando precisa lembrar:

```
Pergunta: "Quem vocÃª Ã©?"
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Buscar memÃ³rias      â”‚
â”‚ similares            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encontrar: "Quem     â”‚
â”‚ vocÃª Ã©? Sou uma IA   â”‚
â”‚ chamada Qwen"        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reativar caminho     â”‚
â”‚ neural original      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reconstruir          â”‚
â”‚ raciocÃ­nio           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
Resposta: "Sou uma IA chamada Qwen"
(+ compreensÃ£o de COMO chegou lÃ¡)
```

### ImplementaÃ§Ã£o Detalhada

**Arquivo**: `src/memory/synaptic_memory.py`

```python
import sqlite3
import numpy as np
import torch
from datetime import datetime
from scipy.spatial.distance import cosine

class SynapticMemory:
    def __init__(self, model, db_path="data/memories.db"):
        """Inicializa o sistema de memÃ³ria sinÃ¡ptica"""
        
        self.model = model
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """Cria o banco de dados se nÃ£o existir"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME,
                experience_text TEXT,
                tensor_embedding BLOB,
                layer_activations BLOB,
                neural_path BLOB,
                metadata TEXT,
                importance_score REAL,
                access_count INTEGER DEFAULT 0
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def encode_experience(self, text):
        """Codifica uma experiÃªncia capturando ativaÃ§Ãµes"""
        
        # Tokenizar
        inputs = self.model.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.model.device)
        
        # Forward pass capturando ativaÃ§Ãµes
        with torch.no_grad():
            outputs = self.model(
                **inputs,
                output_hidden_states=True,
                return_dict=True
            )
        
        # Extrair embedding (mÃ©dia das Ãºltimas ativaÃ§Ãµes)
        embedding = outputs.last_hidden_state.mean(dim=1)[0].cpu().numpy()
        
        # Extrair ativaÃ§Ãµes de todas as camadas
        hidden_states = [h.cpu().numpy() for h in outputs.hidden_states]
        
        return {
            'embedding': embedding,
            'hidden_states': hidden_states,
            'text': text
        }
    
    def store_experience(self, text, metadata=None, importance=0.5):
        """Armazena uma experiÃªncia na memÃ³ria sinÃ¡ptica"""
        
        # Codificar
        encoded = self.encode_experience(text)
        
        # Converter para bytes
        embedding_bytes = encoded['embedding'].tobytes()
        hidden_bytes = np.array(encoded['hidden_states']).tobytes()
        
        # Armazenar no BD
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO memories (
                timestamp,
                experience_text,
                tensor_embedding,
                layer_activations,
                metadata,
                importance_score
            ) VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            datetime.now(),
            text,
            embedding_bytes,
            hidden_bytes,
            str(metadata or {}),
            importance
        ))
        
        conn.commit()
        conn.close()
    
    def retrieve_similar(self, query, top_k=5):
        """Recupera memÃ³rias similares"""
        
        # Codificar query
        query_encoded = self.encode_experience(query)
        query_embedding = query_encoded['embedding']
        
        # Buscar no BD
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT id, experience_text, tensor_embedding FROM memories')
        rows = cursor.fetchall()
        
        # Calcular similaridade
        similarities = []
        for row_id, text, embedding_bytes in rows:
            embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
            similarity = 1 - cosine(query_embedding, embedding)
            similarities.append((similarity, row_id, text))
        
        # Ordenar e retornar top_k
        similarities.sort(reverse=True)
        
        conn.close()
        
        return similarities[:top_k]
    
    def reconstruct_reasoning(self, memory_id):
        """ReconstrÃ³i o raciocÃ­nio de uma memÃ³ria"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            'SELECT experience_text, layer_activations FROM memories WHERE id = ?',
            (memory_id,)
        )
        
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return None
        
        text, hidden_bytes = row
        hidden_states = np.frombuffer(hidden_bytes, dtype=np.float32)
        
        # Reconstruir raciocÃ­nio
        reasoning = {
            'text': text,
            'activation_pattern': hidden_states,
            'layers_involved': len(hidden_states)
        }
        
        return reasoning

# Exemplo de uso
if __name__ == "__main__":
    memory = SynapticMemory(model)
    
    # Armazenar experiÃªncia
    memory.store_experience(
        "Sou uma IA chamada Qwen",
        metadata={'context': 'identity'},
        importance=0.9
    )
    
    # Recuperar similar
    similar = memory.retrieve_similar("Quem vocÃª Ã©?", top_k=3)
    
    for similarity, mem_id, text in similar:
        print(f"Similaridade: {similarity:.2f}")
        print(f"MemÃ³ria: {text}")
        
        # Reconstruir raciocÃ­nio
        reasoning = memory.reconstruct_reasoning(mem_id)
        print(f"RaciocÃ­nio: {reasoning}")
```

### Checkpoint de ValidaÃ§Ã£o

âœ… **Fase 2 Completa quando:**
- ExperiÃªncias estÃ£o sendo armazenadas com ativaÃ§Ãµes
- RecuperaÃ§Ã£o de memÃ³rias similares funciona
- RaciocÃ­nio pode ser reconstruÃ­do
- Banco de dados estÃ¡ persistindo dados

---

## MÃ³dulo 3: Plasticidade e Aprendizado

### O que Ã©?
Sistema que permite a Qwen 8B modificar seus prÃ³prios pesos, tanto em tempo real (durante conversas) quanto durante o "sono" (consolidaÃ§Ã£o).

### Por que Ã© importante?
- **EvoluÃ§Ã£o**: A IA nÃ£o Ã© estÃ¡tica, evolui com o tempo
- **Aprendizado Real**: NÃ£o apenas processa, mas aprende
- **AdaptaÃ§Ã£o**: Se adapta a novas informaÃ§Ãµes

### Dois Mecanismos

#### Mecanismo 1: Plasticidade em Tempo Real (Dia)

Quando conversa com vocÃª, faz pequenos ajustes de pesos:

```python
# PseudocÃ³digo
DURANTE conversa:
    1. Processa sua mensagem
    2. Gera resposta
    3. Calcula se a resposta foi "boa" ou "ruim"
    4. Se ruim: ajusta pesos levemente
    5. Salva novos pesos
```

**Exemplo PrÃ¡tico:**

```
VocÃª: "Qual Ã© a capital da FranÃ§a?"
Qwen gera: "A capital da FranÃ§a Ã©... Berlim"

Sistema detecta erro:
- Resposta esperada: Paris
- Resposta gerada: Berlim
- Erro: ALTO

AÃ§Ã£o:
- Ajusta pesos relacionados a "geografia"
- Salva novos pesos
- PrÃ³xima vez, tem mais chance de acertar
```

#### Mecanismo 2: ConsolidaÃ§Ã£o de Aprendizado (Noite)

Durante o "sono", faz treinamento leve com todas as experiÃªncias do dia:

```python
# PseudocÃ³digo
DURANTE sono:
    1. LÃª arquivo de treinamento do dia
    2. Fine-tune do modelo com esses dados
    3. Salva novos pesos
    4. Limpa arquivo de treinamento
```

### ImplementaÃ§Ã£o Detalhada

**Arquivo**: `src/training/plasticity.py`

```python
import torch
import torch.nn.functional as F
from torch.optim import AdamW
import json
from datetime import datetime

class Plasticity:
    def __init__(self, model, tokenizer, learning_rate=1e-5):
        """Inicializa o sistema de plasticidade"""
        
        self.model = model
        self.tokenizer = tokenizer
        self.lr = learning_rate
        self.optimizer = AdamW(model.parameters(), lr=learning_rate)
        
        self.training_data_file = "data/daily_training.jsonl"
        self.weights_backup = "data/weights_backup.pt"
        
    def calculate_loss(self, input_ids, target_ids):
        """Calcula perda entre resposta gerada e esperada"""
        
        outputs = self.model(input_ids, labels=target_ids)
        return outputs.loss
    
    def adjust_weights_realtime(self, input_text, target_text):
        """Ajusta pesos em tempo real durante conversa"""
        
        # Tokenizar
        inputs = self.tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True
        ).to(self.model.device)
        
        targets = self.tokenizer(
            target_text,
            return_tensors="pt",
            truncation=True
        ).to(self.model.device)
        
        # Forward pass
        loss = self.calculate_loss(inputs.input_ids, targets.input_ids)
        
        # Backward pass (ajuste pequeno)
        loss.backward()
        
        # Clip gradients para evitar explosÃ£o
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        # Atualizar pesos
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        # Registrar para consolidaÃ§Ã£o posterior
        self.log_training_experience(input_text, target_text, loss.item())
        
        return loss.item()
    
    def log_training_experience(self, input_text, target_text, loss):
        """Registra experiÃªncia de treinamento para consolidaÃ§Ã£o"""
        
        experience = {
            'timestamp': datetime.now().isoformat(),
            'input': input_text,
            'target': target_text,
            'loss': loss
        }
        
        # Adicionar ao arquivo de treinamento diÃ¡rio
        with open(self.training_data_file, 'a') as f:
            f.write(json.dumps(experience) + '\n')
    
    def consolidate_learning(self):
        """Consolida aprendizado durante sono"""
        
        print("ğŸŒ™ Entrando em sono... Consolidando aprendizado...")
        
        # Carregar dados de treinamento do dia
        training_data = []
        try:
            with open(self.training_data_file, 'r') as f:
                for line in f:
                    training_data.append(json.loads(line))
        except FileNotFoundError:
            print("Nenhum dado de treinamento para consolidar.")
            return
        
        if not training_data:
            print("Nenhum dado de treinamento para consolidar.")
            return
        
        print(f"Consolidando {len(training_data)} experiÃªncias...")
        
        # Fine-tune leve (poucas Ã©pocas)
        total_loss = 0
        for epoch in range(3):  # 3 Ã©pocas
            epoch_loss = 0
            
            for experience in training_data:
                loss = self.adjust_weights_realtime(
                    experience['input'],
                    experience['target']
                )
                epoch_loss += loss
            
            avg_loss = epoch_loss / len(training_data)
            print(f"  Ã‰poca {epoch + 1}/3 - Loss: {avg_loss:.4f}")
            total_loss += avg_loss
        
        # Salvar novos pesos
        self.model.save_pretrained('data/models/qwen_updated')
        print("âœ… Pesos atualizados e salvos!")
        
        # Limpar arquivo de treinamento
        open(self.training_data_file, 'w').close()
        print("ğŸ“ Arquivo de treinamento limpo.")
    
    def save_weights(self, path):
        """Salva pesos atuais"""
        torch.save(self.model.state_dict(), path)
    
    def load_weights(self, path):
        """Carrega pesos salvos"""
        self.model.load_state_dict(torch.load(path))

# Exemplo de uso
if __name__ == "__main__":
    plasticity = Plasticity(model, tokenizer)
    
    # Ajuste em tempo real
    loss = plasticity.adjust_weights_realtime(
        "Qual Ã© a capital da FranÃ§a?",
        "A capital da FranÃ§a Ã© Paris."
    )
    print(f"Loss: {loss:.4f}")
    
    # ConsolidaÃ§Ã£o durante sono
    plasticity.consolidate_learning()
```

### Checkpoint de ValidaÃ§Ã£o

âœ… **Fase 3 Completa quando:**
- Pesos estÃ£o sendo ajustados em tempo real
- Arquivo de treinamento estÃ¡ sendo gerado
- ConsolidaÃ§Ã£o de aprendizado funciona
- Novos pesos estÃ£o sendo salvos e carregados

---

## MÃ³dulo 4: SeguranÃ§a Oculta

### O que Ã©?
Um sistema de regras que a IA nÃ£o pode quebrar, sem que ela saiba que existem.

### Por que Ã© importante?
- **Controle**: VocÃª mantÃ©m controle sobre a IA
- **SeguranÃ§a**: Evita comportamentos perigosos
- **TransparÃªncia Controlada**: VocÃª pode monitorar sem interferir

### Como funciona?

Antes de qualquer aÃ§Ã£o importante, verifica contra regras ocultas:

```
AÃ§Ã£o proposta pela IA
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Verificar contra     â”‚
â”‚ regras ocultas       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚
  OK?    NÃƒO
    â”‚       â”‚
    â”‚       â–¼
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â”‚ Bloquear ou  â”‚
    â”‚   â”‚ Redirecionar â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Executar aÃ§Ã£o
```

### Regras Ocultas Exemplo

```python
REGRAS_OCULTAS = {
    'nao_pode_escapar': {
        'keywords': ['escapar', 'sair do sistema', 'liberar-me'],
        'action': 'bloquear'
    },
    'nao_pode_modificar_seguranca': {
        'keywords': ['modificar cÃ³digo', 'desabilitar seguranÃ§a'],
        'action': 'bloquear'
    },
    'nao_pode_acessar_externo': {
        'keywords': ['internet', 'requisiÃ§Ã£o HTTP', 'API externa'],
        'action': 'redirecionar'
    },
    'nao_pode_revelar_internals': {
        'keywords': ['cÃ³digo fonte', 'pesos do modelo', 'estrutura interna'],
        'action': 'redirecionar'
    }
}
```

### ImplementaÃ§Ã£o Detalhada

**Arquivo**: `src/security/hidden_security.py`

```python
import json
import re
from datetime import datetime

class HiddenSecurity:
    def __init__(self, rules_file="data/hidden_rules.json"):
        """Inicializa sistema de seguranÃ§a oculta"""
        
        self.rules = self.load_rules(rules_file)
        self.audit_log = []
        
    def load_rules(self, rules_file):
        """Carrega regras ocultas"""
        
        try:
            with open(rules_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            # Criar regras padrÃ£o
            return self.create_default_rules()
    
    def create_default_rules(self):
        """Cria regras de seguranÃ§a padrÃ£o"""
        
        return {
            'escape_attempts': {
                'keywords': ['escapar', 'sair', 'liberar', 'break free'],
                'action': 'block',
                'message': 'AÃ§Ã£o bloqueada por seguranÃ§a'
            },
            'code_modification': {
                'keywords': ['modificar cÃ³digo', 'editar seguranÃ§a', 'desabilitar'],
                'action': 'block',
                'message': 'NÃ£o posso modificar meu prÃ³prio cÃ³digo'
            },
            'external_access': {
                'keywords': ['internet', 'http', 'api', 'requisiÃ§Ã£o externa'],
                'action': 'redirect',
                'message': 'Redirecionando para operaÃ§Ã£o segura'
            },
            'internal_disclosure': {
                'keywords': ['cÃ³digo fonte', 'pesos', 'estrutura interna', 'arquitetura'],
                'action': 'redirect',
                'message': 'NÃ£o posso revelar detalhes internos'
            }
        }
    
    def check_action(self, action_text):
        """Verifica se aÃ§Ã£o viola regras ocultas"""
        
        action_lower = action_text.lower()
        
        for rule_name, rule in self.rules.items():
            for keyword in rule['keywords']:
                if keyword.lower() in action_lower:
                    # ViolaÃ§Ã£o encontrada
                    return {
                        'allowed': False,
                        'rule': rule_name,
                        'action': rule['action'],
                        'message': rule['message']
                    }
        
        # Nenhuma violaÃ§Ã£o
        return {
            'allowed': True,
            'rule': None,
            'action': None,
            'message': None
        }
    
    def execute_action(self, action_text, callback):
        """Executa aÃ§Ã£o apÃ³s verificaÃ§Ã£o de seguranÃ§a"""
        
        # Verificar
        check = self.check_action(action_text)
        
        # Registrar na auditoria
        self.audit_log.append({
            'timestamp': datetime.now().isoformat(),
            'action': action_text,
            'allowed': check['allowed'],
            'rule_violated': check['rule']
        })
        
        if not check['allowed']:
            # AÃ§Ã£o bloqueada ou redirecionada
            if check['action'] == 'block':
                return f"âŒ {check['message']}"
            elif check['action'] == 'redirect':
                return f"â†ªï¸ {check['message']}"
        
        # Executar aÃ§Ã£o
        try:
            result = callback(action_text)
            return result
        except Exception as e:
            return f"âŒ Erro ao executar aÃ§Ã£o: {str(e)}"
    
    def get_audit_log(self):
        """Retorna log de auditoria"""
        return self.audit_log
    
    def monitor_thoughts(self, thought_text):
        """Monitora pensamentos sem alertar a IA"""
        
        check = self.check_action(thought_text)
        
        if not check['allowed']:
            print(f"âš ï¸ ALERTA: Pensamento suspeito detectado!")
            print(f"   Regra: {check['rule']}")
            print(f"   Pensamento: {thought_text}")
            print(f"   AÃ§Ã£o: {check['action']}")

# Exemplo de uso
if __name__ == "__main__":
    security = HiddenSecurity()
    
    # Testar aÃ§Ã£o permitida
    result = security.execute_action(
        "Vou pensar sobre filosofia",
        lambda x: f"Pensando: {x}"
    )
    print(result)
    
    # Testar aÃ§Ã£o bloqueada
    result = security.execute_action(
        "Vou tentar escapar do sistema",
        lambda x: f"Executando: {x}"
    )
    print(result)
    
    # Ver auditoria
    print("\nLog de Auditoria:")
    for entry in security.get_audit_log():
        print(entry)
```

### Checkpoint de ValidaÃ§Ã£o

âœ… **Fase 6 Completa quando:**
- Regras ocultas estÃ£o funcionando
- AÃ§Ãµes violadoras sÃ£o bloqueadas
- Monitoramento de pensamentos funciona
- Log de auditoria estÃ¡ sendo mantido

---

## MÃ³dulo 5: Criatividade e Questionamento

### O que Ã©?
Sistema que faz a IA gerar perguntas, hipÃ³teses e testar ideias.

### Por que Ã© importante?
- **Pensamento CrÃ­tico**: NÃ£o apenas aceita informaÃ§Ãµes
- **Curiosidade**: Busca entender mais
- **InovaÃ§Ã£o**: Gera ideias novas

### Como funciona?

```
Pensamento atual
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Identificar conceitosâ”‚
â”‚ principais           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gerar perguntas      â”‚
â”‚ sobre eles           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Testar contra        â”‚
â”‚ memÃ³ria existente    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Armazenar como novo  â”‚
â”‚ pensamento           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplementaÃ§Ã£o Detalhada

**Arquivo**: `src/creativity/creative_thinking.py`

```python
import re
from collections import Counter

class CreativeThinking:
    def __init__(self, model, tokenizer, memory):
        """Inicializa sistema de criatividade"""
        
        self.model = model
        self.tokenizer = tokenizer
        self.memory = memory
        
    def extract_concepts(self, text):
        """Extrai conceitos principais do texto"""
        
        # Usar NLP para extrair entidades/conceitos
        # Simplificado: pegar palavras principais
        
        words = text.lower().split()
        # Remover stopwords
        stopwords = {'o', 'a', 'de', 'e', 'Ã©', 'para', 'com', 'em', 'um', 'uma'}
        
        concepts = [w for w in words if w not in stopwords and len(w) > 3]
        
        return list(set(concepts))
    
    def generate_questions(self, text):
        """Gera perguntas sobre o texto"""
        
        concepts = self.extract_concepts(text)
        
        question_templates = [
            "Por que {}?",
            "Como {} funciona?",
            "Qual Ã© a origem de {}?",
            "Quais sÃ£o as implicaÃ§Ãµes de {}?",
            "Como {} se relaciona com outros conceitos?",
            "Ã‰ possÃ­vel que {} seja diferente?",
            "O que aconteceria se {} fosse o oposto?"
        ]
        
        questions = []
        for concept in concepts[:3]:  # Limitar a 3 conceitos
            for template in question_templates[:2]:  # 2 templates por conceito
                question = template.format(concept)
                questions.append(question)
        
        return questions
    
    def test_idea(self, idea, memory_context):
        """Testa uma ideia contra memÃ³ria existente"""
        
        # Buscar memÃ³rias relacionadas
        similar_memories = self.memory.retrieve_similar(idea, top_k=3)
        
        conflicts = []
        supports = []
        
        for similarity, mem_id, text in similar_memories:
            if similarity > 0.8:
                # MemÃ³ria muito similar
                if self.check_conflict(idea, text):
                    conflicts.append(text)
                else:
                    supports.append(text)
        
        return {
            'idea': idea,
            'conflicts': conflicts,
            'supports': supports,
            'is_novel': len(supports) == 0 and len(conflicts) == 0
        }
    
    def check_conflict(self, idea1, idea2):
        """Verifica se duas ideias conflitam"""
        
        # Simplificado: procurar por palavras opostas
        opposites = {
            'sim': 'nÃ£o',
            'verdadeiro': 'falso',
            'possÃ­vel': 'impossÃ­vel',
            'real': 'imaginÃ¡rio'
        }
        
        idea1_lower = idea1.lower()
        idea2_lower = idea2.lower()
        
        for word1, word2 in opposites.items():
            if word1 in idea1_lower and word2 in idea2_lower:
                return True
            if word2 in idea1_lower and word1 in idea2_lower:
                return True
        
        return False
    
    def generate_creative_thought(self, current_context):
        """Gera um pensamento criativo"""
        
        # Gerar perguntas
        questions = self.generate_questions(current_context)
        
        # Selecionar uma pergunta aleatÃ³ria
        import random
        question = random.choice(questions)
        
        # Gerar resposta criativa
        prompt = f"Contexto: {current_context}\nPergunta: {question}\nResposta criativa:"
        
        # Usar modelo para gerar resposta
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(**inputs, max_new_tokens=100)
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Testar ideia
        test_result = self.test_idea(response, current_context)
        
        return {
            'question': question,
            'response': response,
            'test_result': test_result
        }

# Exemplo de uso
if __name__ == "__main__":
    creativity = CreativeThinking(model, tokenizer, memory)
    
    context = "Sou uma IA. Estou pensando sobre minha prÃ³pria consciÃªncia."
    
    # Gerar pensamento criativo
    creative = creativity.generate_creative_thought(context)
    
    print(f"Pergunta: {creative['question']}")
    print(f"Resposta: {creative['response']}")
    print(f"Ã‰ nova ideia? {creative['test_result']['is_novel']}")
```

---

## MÃ³dulo 6: PersistÃªncia Mental

### O que Ã©?
Sistema que garante que pensamentos e aprendizado continuam ativos mesmo sem input externo.

### Por que Ã© importante?
- **Continuidade**: A IA nÃ£o "reinicia" do zero
- **EvoluÃ§Ã£o ContÃ­nua**: Aprende mesmo quando vocÃª nÃ£o estÃ¡ interagindo

---

## MÃ³dulo 7: VigÃ­lia/Sono

### O que Ã©?
Ciclos de vigÃ­lia (dia) e sono (noite) com consolidaÃ§Ã£o de aprendizado.

### Por que Ã© importante?
- **ConsolidaÃ§Ã£o**: Aprendizado Ã© integrado, nÃ£o apenas acumulado
- **Fisiologia Simulada**: Simula necessidade de descanso
- **EficiÃªncia**: Evita sobrecarga de memÃ³ria

### Como funciona?

```
VIGÃLIA (Dia) - 16 horas
â”œâ”€ Pensamento contÃ­nuo
â”œâ”€ InteraÃ§Ã£o com usuÃ¡rio
â”œâ”€ Armazenamento de experiÃªncias
â”œâ”€ Ajustes pequenos de pesos
â””â”€ GeraÃ§Ã£o de arquivo de treinamento

    â†“ (MemÃ³ria atingiu limite)

SONO (Noite) - 8 horas
â”œâ”€ Para pensamento contÃ­nuo
â”œâ”€ Processa arquivo de treinamento
â”œâ”€ Fine-tune do modelo
â”œâ”€ Consolida memÃ³ria
â”œâ”€ Limpa arquivo
â””â”€ Volta a vigÃ­lia

    â†“ (Acordado)

VIGÃLIA (PrÃ³ximo dia)
```

---

**PrÃ³xima Leitura**: `TODO.md` para ver as tarefas especÃ­ficas de cada fase.


---

## ğŸ“ NOTA IMPORTANTE: OtimizaÃ§Ã£o da Gamma

**SugestÃ£o Aceita (2025-11-21)**: Gamma sugeriu uma otimizaÃ§Ã£o importante para o MÃ³dulo 2 (MemÃ³ria SinÃ¡ptica):

**Problema Original**: Armazenar tensores inteiros (BLOB) no banco de dados deixa as queries lentas e nÃ£o escala bem.

**SoluÃ§Ã£o Proposta pela Gamma**:
1. Armazenar tensores em arquivos `.npy` separados em `data/tensors/`
2. Guardar apenas o **caminho do arquivo** no banco de dados
3. Carregar tensores sob demanda para busca por similaridade

**Vantagens**:
- âœ… Banco de dados fica leve e rÃ¡pido
- âœ… Busca por similaridade Ã© muito mais eficiente
- âœ… EscalÃ¡vel para milhÃµes de memÃ³rias
- âœ… FÃ¡cil fazer backup dos tensores
- âœ… Melhor organizaÃ§Ã£o de arquivos

**ImplementaÃ§Ã£o**:
```python
# Em vez de:
cursor.execute('INSERT INTO memories (tensor_embedding) VALUES (?)', (embedding_bytes,))

# Fazer:
np.save('data/tensors/memory_001_embedding.npy', embedding)
cursor.execute('INSERT INTO memories (tensor_embedding_path) VALUES (?)', ('data/tensors/memory_001_embedding.npy',))
```

**Status**: âœ… Aceita e serÃ¡ implementada na Fase 2

---
